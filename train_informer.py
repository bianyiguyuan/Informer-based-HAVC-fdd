import os
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, random_split
from timeseries_dataset import TimeSeriesDataset
from model import Informer

BATCH_SIZE = 2700

DATASET_DIR = "LBNL_FDD_Dataset_SDAHU_all/LBNL_FDD_Dataset_SDAHU/"
FAULT_PATH = [os.path.join(DATASET_DIR, f) for f in os.listdir(DATASET_DIR) 
               if f.endswith(".csv") and "AHU_annual" not in f]

def load_and_preprocess(csv_file, features):
    df = pd.read_csv(csv_file)
    df = df[features]
    df['Timestamp'] = pd.date_range(start='2023-01-01', periods=len(df), freq='T')
    df.set_index('Timestamp', inplace=True)

    df['month_of_year'] = (df.index.month - 1) / 11.0  
    df['day_of_month'] = (df.index.day - 1) / 30.0  
    df['weekday'] = df.index.dayofweek / 6.0  
    df['hour_of_day'] = df.index.hour / 23.0  
    df['minute_of_hour'] = df.index.minute / 59.0  

    df = df.fillna(method='ffill').fillna(method='bfill')

    # Normalize data
    scaler = MinMaxScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)

    return df_scaled, scaler


def train_informer_main():
    # 1) æŒ‡å®šæ•°æ®æ–‡ä»¶å’Œç‰¹å¾åˆ—
    csv_file = 'LBNL_FDD_Dataset_SDAHU_all/LBNL_FDD_Dataset_SDAHU/AHU_annual.csv'  
    def get_features_from_csv(file_path):

        df = pd.read_csv(file_path, nrows=5)
        features = list(df.columns)
        time_columns = ["timestamp", "time", "datetime", "date"] 
        features = [col for col in features if col.lower() not in time_columns]

        return features

    features = get_features_from_csv(csv_file)

    # 2) åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    df_scaled, _ = load_and_preprocess(csv_file, features)
    print("Data shape after scaling:", df_scaled.shape)
    
    # 3) æ„å»º Dataset
    seq_len = 48
    label_len = 24
    pred_len = 24
    
    dataset = TimeSeriesDataset(
        data=df_scaled, 
        seq_len=seq_len,
        label_len=label_len,
        pred_len=pred_len
    )
    
    # 4) åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
    total_size = len(dataset)
    train_size = int(total_size * 0.8)
    val_size = total_size - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    
    val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False)

    
    # 5) å®ä¾‹åŒ– Informer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("CUDA Available:", torch.cuda.is_available())
    print("CUDA Device Count:", torch.cuda.device_count())
    if torch.cuda.is_available():
        print("Current CUDA Device:", torch.cuda.current_device())
        print("CUDA Device Name:", torch.cuda.get_device_name(torch.cuda.current_device()))

    n_features = len(features) + 5  # é¢„æµ‹æ‰€æœ‰ä¼ æ„Ÿå™¨
    
    model = Informer(
        enc_in=n_features,  # è¾“å…¥ç‰¹å¾æ•°
        dec_in=n_features,  # Decoder ä¹Ÿä½¿ç”¨ç›¸åŒç‰¹å¾æ•°
        c_out=n_features,   # é¢„æµ‹æ‰€æœ‰ä¼ æ„Ÿå™¨
        seq_len=seq_len,
        label_len=label_len,
        out_len=pred_len,   # é¢„æµ‹æ­¥é•¿
        d_model=512,
        n_heads=8,
        e_layers=3,
        d_layers=2,
        d_ff=512,
        dropout=0.05,
        attn='prob', 
        embed='fixed', 
        freq='h',   
        activation='gelu',
        output_attention=False,
        distil=True,
        mix=True,
        device=device
    ).to(device)
    
    # 6) å®šä¹‰æŸå¤±ä¸ä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    # 7) è®­ç»ƒå¾ªç¯
    epochs = 5
    for epoch in range(epochs):
        train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)
        print(train_loader)

        if not check_none_values(train_loader):
            print("ğŸš¨ Training aborted due to None values in DataLoader!")
            return  # ç»ˆæ­¢è®­ç»ƒï¼Œé¿å…é”™è¯¯ä¼ æ’­

        model.train()
        train_loss_sum = 0.0
        for i, (x_enc, x_dec, y) in enumerate(train_loader):
            x_enc, x_dec, y = x_enc.to(device), x_dec.to(device), y.to(device)
            # æå–æ—¶é—´ä¿¡æ¯ï¼ˆå‡è®¾æœ€åä¸¤åˆ—æ˜¯æ—¶é—´ç‰¹å¾ï¼‰
            x_mark_enc = x_enc[:, :, -5:].to(device)  # å– 5 ç»´æ—¶é—´ç‰¹å¾
            x_mark_dec = x_dec[:, :, -5:].to(device)  # å– 5 ç»´æ—¶é—´ç‰¹å¾


            preds = model(x_enc, x_mark_enc, x_dec, x_mark_dec)  # ä¼ å…¥æ—¶é—´ä¿¡æ¯
            
            loss = criterion(preds, y)  # è®¡ç®—æ‰€æœ‰ä¼ æ„Ÿå™¨çš„é¢„æµ‹è¯¯å·®
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss_sum += loss.item()

            print(f"Epoch {epoch+1}, Batch {i+1}: Loss={loss.item():.6f}")
        
        train_loss_avg = train_loss_sum / len(train_loader)
        print(f"Epoch {epoch+1} - Total Train Loss: {train_loss_sum:.4f}, Average: {train_loss_avg:.4f}")
        
        # éªŒè¯
        model.eval()
        val_loss_sum = 0.0
        with torch.no_grad():
            for j, (x_enc, x_dec, y) in enumerate(val_loader):
                x_enc, x_dec, y = x_enc.to(device), x_dec.to(device), y.to(device)
                x_mark_enc = x_enc[:, :, -5:]
                x_mark_dec = x_dec[:, :, -5:]
                preds = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
                loss_val = criterion(preds, y)
                val_loss_sum += loss_val.item()
                print(f"Validation Batch {j+1}: Loss={loss_val.item():.6f}")

        val_loss_avg = val_loss_sum / len(val_loader)
        print(f"Epoch {epoch+1}/{epochs}, Train Loss={train_loss_avg:.4f}, Val Loss={val_loss_avg:.4f}")
    
    # 8) è®­ç»ƒç»“æŸï¼Œå¯ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), "informer_sdahu_"+ str(BATCH_SIZE) +".pth")
    print("Model saved: informer_sdahu_"+ str(BATCH_SIZE) +".pth")

def check_none_values(data_loader):
    print("Checking DataLoader for None values...")
    for batch_idx, (x_enc, x_dec, y) in enumerate(data_loader):
        if x_enc is None or x_dec is None or y is None:
            print(f"None value detected in batch {batch_idx}!")
            return False
    print("DataLoader check passed, no None values detected.")
    return True

if __name__ == "__main__":
    train_informer_main()
